{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules\n",
    "\n",
    "#### In this section, we will learn about how to use assocition rules in python and how to filter data based on different metrics of association rules.\n",
    "\n",
    "\n",
    "Following libraries are used for association rules:\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import FP-growth and Apriori modules, TransactionEncoder module and association module from mlxtend\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules as arule\n",
    "from mlxtend.frequent_patterns import fpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Repair.csv\n",
    "\n",
    "We use Repair.csv file as a data set for finding frequent item sets. FP-growth and Apriori algorithms are used for finding frequent itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file 'Repair.csv' and change the data format for applying algorithms\n",
    "\n",
    "data_set = []\n",
    "\n",
    "with open(\"Repair.csv\") as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        data_set.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-grwoth algorithm\n",
    "\n",
    "We use Repair.csv file as a data set for finding frequent item sets. FP-growth algorithm is used for finding frequent itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn to use TransactionEncoder module to convert an array to DataFrame for FP-growth algorithm in mlxtend\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data_set).transform(data_set)\n",
    "data = pd.DataFrame(te_ary, columns = te.columns_)\n",
    "data.tail(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets=fpgrowth(data, min_support=0.3, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets_filtered = frequent_itemsets.loc[(frequent_itemsets['length'] > 3) & (frequent_itemsets['support'] > 0.3)]   \n",
    "frequent_itemsets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori algorithm\n",
    "\n",
    "We use Repair.csv file as a data set for finding frequent item sets. Apriori algorithm is used to find frequent itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn to use TransactionEncoder module to convert an array to DataFrame for Apriori algorithm in mlxtend\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data_set).transform(data_set)\n",
    "data = pd.DataFrame(te_ary, columns = te.columns_)\n",
    "data.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(data, min_support = 0.3, use_colnames = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data based on metrics of association rules\n",
    "In python you can filter frequent itemsets based on different metrics such as support, confidence, and lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn to use the association rule algorithm from mlxtend and filter data based on one metric.\n",
    "\n",
    "rules_association =arule(frequent_itemsets, metric = 'lift', min_threshold = 0.8)\n",
    "rules_association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Change the metric to lift and support. Investigate the effect of that on the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding qualified frequent itemsets using association rules\n",
    "You can use association rules to find qualified itemsets for different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:  Find frequent item sets with minimum support of 0.2. Store them in frequent_itemsets variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "frequent_itemsets = apriori(data, min_support = 0.2, use_colnames = True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering itemsets based on length and metrics of assosciation rules\n",
    "In this section, you will learn how to filter frequent item sets based on length of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column named 'length' in 'frequent_itemsets' which indicates the number of items in each frequent itemset.\n",
    "\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# Filter out the frequent itemsets which have a length longer than 2 and a support bigger than 0.3. \n",
    "\n",
    "# Store these found itemsets in variable 'frequent_itemsets_filtered'.\n",
    "\n",
    "frequent_itemsets_filtered = frequent_itemsets.loc[(frequent_itemsets['length'] > 2) & (frequent_itemsets['support'] > 0.3)]   \n",
    "frequent_itemsets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating selective metrics of association rules in one table\n",
    " In this section, you will learn how to show selective metrics of association rules in one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine association rules from the discovered frequent itemsets stored in variable 'frequent_itemsets', set minimum confidence to 0.5.\n",
    "\n",
    "# Store the discovered rules in variable 'rules_association'.\n",
    "\n",
    "rules_association =arule(frequent_itemsets, metric = 'confidence', min_threshold = 0.5)\n",
    "\n",
    "# Filter out the rules with lift larger than 1 and support larger than 0.4, store the discovered rules in variable 'filtered_rules'.\n",
    "\n",
    "filtered_rules = rules_association.loc[(rules_association['lift'] > 1) & (rules_association['support'] > 0.4)]     \n",
    "\n",
    "# Show the columns 'antecedents', 'consequents', 'support', 'confidence' and 'lift' of variable 'filtered_rules' \n",
    "\n",
    "filtered_rules[['support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
