{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining\n",
    "In the text mining lectures, we have discussed how to preprocess textual data and create a model. We have also demonstrated how a model can be utilized with different data science techniques.\n",
    "\n",
    "Now, we are putting the acquired knowledge into practice using Python. \n",
    "The main task is to implement required text preprocessing steps and solve problems using techniques demonstrated in the lecture.\n",
    "\n",
    "The main packages for the text mining are `scikit-learn` (https://scikit-learn.org/stable/) and `nltk` (https://www.nltk.org/). `Scikit-learn` is a popular machine learning library featuring algorithms that also provide a range of useful utilities for text mining. `Nltk` is intended to support your text mining research and contains essential features for natural language processing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with basic text preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation. \n",
    "Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence. \"\"\"\n",
    "tokenized_sent=sent_tokenize(text)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list, we can find redundancies, such as punctuation.\n",
    "\n",
    "We have an option to tokenize words and remove punctuation at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove puntuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "tokenized_words_no_puntuation = tokenizer.tokenize(text)\n",
    "print(tokenized_words_no_puntuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now examine the text and find out the most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_words_no_puntuation)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most common words with their frequencies\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot and the output before, we observe that a few words are capitalized (\"Turning\", \"NLP\") and there are words with a low research value - stop words, such as \"and\", \"the\", \"a\" and \"of\".\n",
    "That is why it is important to clean the data - to low case the text and remove punctuation and stop words during the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "lower_text = text.lower()\n",
    "print (lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Now it is your turn.\n",
    "Please preprocess the following piece of text.\n",
    "\n",
    "\"\"\"\n",
    "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.\n",
    "Data science is a \"concept to unify statistics, data analysis and their related methods\" in order to \"understand and analyze actual phenomena\" with data.\n",
    "Natural language processing is an important part of data science.\n",
    "\"\"\"\n",
    "\n",
    "First, you need to low case it. Then remove punctuation and tokenize (can be done as one step). \n",
    "\n",
    "Print the preprocessed sentences (lowcase, no punctuation, no stop words)and build a words frequency graph with the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please give your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon normalization: stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words=[]\n",
    "for w in filtered_words:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "#print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference betweeb stemming and lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"studying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming implies removing suffixes and obtaining word roots. \n",
    "There are different kinds of stemmers, such as SnowballStemmer and PorterStemmer https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    " \n",
    "snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "example = 'Process mining is a family of techniques in the field of process management that support the analysis of business processes based on event logs. During process mining, specialized data mining algorithms are applied to event log data in order to identify trends, patterns and details contained in event logs recorded by an information system. Process mining aims to improve process efficiency and understanding of processes.'\n",
    "wordlist = example.split(' ')\n",
    "print(example)\n",
    "print()\n",
    "print(' '.join([snowball_stemmer.stem(word) for word in wordlist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stemming procedure does not always imply just cutting a part of a word. Modern stemmers also modify the root of the word. This nevertheless cannot be considered as lemmatization, because converging to lemmas is not the goal of stemming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization transforms a token to its base form, a lemma. In nltk, you can use the WordNet lemmatizer. Let's see the results of lemmatization comparing to stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example = 'Process mining is a family of techniques in the field of process management that support the analysis of business processes based on event logs. During process mining, specialized data mining algorithms are applied to event log data in order to identify trends, patterns and details contained in event logs recorded by an information system. Process mining aims to improve process efficiency and understanding of processes.'\n",
    "wordlist = example.split(' ')\n",
    "print(example)\n",
    "print()\n",
    "print(' '.join([snowball_stemmer.stem(word) for word in wordlist]))\n",
    "print()\n",
    "print(' '.join([lemmatizer.lemmatize(word) for word in wordlist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that lemmatization is more complicated comparing to stemming. Lemmatizers tend to be more conservative than stemmers.\n",
    "\n",
    "It is useful that the WordNet stemmer applies a built-in morphy function. It passes a part-of-speech tag as a parameter, and depending on it, a lemma is identified. The default behaviour of the WordNet lemmatizer is to consider everything a NOUN. Let's see how the result changes passing a VERB tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('loving'))\n",
    "print(lemmatizer.lemmatize('loving', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) tagging\n",
    "The POS tagging looks for relationships within a sentence and assigns a corresponding tag to each word.\n",
    "\n",
    "You can read more about POS tagging here https://www.nltk.org/book/ch05.html\n",
    "\n",
    "And the POS tag list can be found here https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification task\n",
    "\n",
    "Text classification is a very popular task in text mining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "\n",
    "The first step is to obtain the data for analysis. \n",
    "Here, we are using again the dataset `20 newsgroups` that is organized into 20 different newsgroups, each corresponding to a different topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training set part of 20newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the topics that would be the target attribute, which is required for supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the categories(target attribute)\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragment of a document\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "In order to perform machine learning on text documents, we first need to turn a text content into numerical feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and the bag-of-words (BoW) model\n",
    "The `CountVectorizer` method can be used to directly transform the dataset to the BoW model. This will first tokenize the text into words, and then create a vector space with one dimension for every word in the dictionary. Finally, it translates the documents into count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and construction of the bag-of-words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build our first classifier using the count vector space. To do so, we are using the `pipeline` function of `scikit-learn`: we specify a sequence of operations to be performed on the data. In this case, we apply the `CountVectorizer` and then the SVM classifier with linear kernel and stochastic gradient descent as solver for the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v1: tokenization, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind this result, we will try to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf scoring - from occurrence counts to normalized frequencies\n",
    "After that, we can try to improve our results by introducing a tf-idf scoring step in the pipeline. This is required because longer documents have higher average count values than shorter documents, even though they might talk about the same topics. \n",
    "\n",
    "In order to deal with the issue we can apply `TfidfTransformer` to convert the values of the vector from simple counts (tf) to tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the tf-idf score matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add it direcly to our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v2: tokenization, tf-idf scoring, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tf-idf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieve an accuracy of 82% that is higher than in the previous test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word removal\n",
    "Another way to improve the model is to remove stop words. `CountVectorizer` has an integrated stop list, and we can add an option to remove the stop words when we are building the vector space. By adding a clause (stop_words='english'), the `pipeline` will include stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v3: tokenization, stopword removal, tf-idf scoring, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tf-idf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Can you suggest other steps to improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec\n",
    "\n",
    "In the second text mining lecture, you have learned about advanced models and techniques to analyze texts, such as n-grams and word2vec/doc2vec. In this part, we are going to study how one can train a do2vec model and use it for text classification.\n",
    "\n",
    "For this purpose, we are going to use the 20newsgroups corpus again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 20newsgroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, targets are not strings but numbers. The target_names attribute allows to fetch the list of labels: targets are indexes in this list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "For this test, we are _not_ going to normalize the text, we will just tokenize it. \n",
    "\n",
    "The gensim tool (https://radimrehurek.com/gensim/) gensim.utils.simple_preprocess tokenizes a text, puts everything in lowercase and eliminates punctuation.\n",
    "\n",
    "Gensim's doc2vec requires a list of TaggedDocument objects as a input. A TaggedDocument is created with two parameters: words, which has to be a list of strings (tokens); and tags, which has to be a list of strings (labels). \n",
    "\n",
    "In our case, labels are unique. That is why we have to use a list with just one element - targets are lists because TaggedDocument also supports multilabel classification. Using the syntax, we fetch the string label for each document and we create TaggedDocuments. We do this for the training and the test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, normalizing, and creating lists of TaggedDocument objects\n",
    "import gensim\n",
    "\n",
    "twenty_train_tagged = []\n",
    "twenty_test_tagged = []\n",
    "\n",
    "for i in range (0, len(twenty_train.data)):\n",
    "    twenty_train_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(twenty_train.data[i]), tags=[twenty_train.target_names[twenty_train.target[i]]]))\n",
    "\n",
    "for i in range (0, len(twenty_test.data)):\n",
    "    twenty_test_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(twenty_test.data[i]), tags=[twenty_test.target_names[twenty_test.target[i]]]))\n",
    "\n",
    "print(repr(twenty_train_tagged[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the calculations, let's fetch the number of cores the machine has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary\n",
    "\n",
    "At this point, we are ready to train our doc2vec model. The first thing to do is to create the vocabulary. This allows to determine the size of input and output and build the one-hot encoding for tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "#doc2vec_model = Doc2Vec(dm=0, vector_size=40, min_count=2, workers=cores)\n",
    "doc2vec_model = Doc2Vec(dm=0, vector_size=40, workers=cores)\n",
    "doc2vec_model.build_vocab([x for x in tqdm(twenty_train_tagged)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining a document embedding\n",
    "\n",
    "After creating the vocabulary, it is time to train the encoding neural network that will provide the representation. The hyper parameters are the regular ones for neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the doc2vec model\n",
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(30):\n",
    "    doc2vec_model.train(utils.shuffle([x for x in tqdm(twenty_train_tagged)]), total_examples=len(twenty_train_tagged), epochs=1)\n",
    "    doc2vec_model.alpha -= 0.002\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a document features vector\n",
    "\n",
    "Once trained the doc2vec representation model, we can use it to convert documents into fixed-length vectors in order to use these vectors in a classifier. For that purpose we can use the infer_vector method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the feature vector for the classifier\n",
    "def vec_for_learning(model, docs):\n",
    "    doc2vec_vectors = [model.infer_vector(doc.words) for doc in docs]\n",
    "    targets = [doc.tags[0] for doc in docs]\n",
    "    return doc2vec_vectors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating docs into vectors for training and test set\n",
    "X_train, y_train = vec_for_learning(doc2vec_model, twenty_train_tagged)\n",
    "X_test, y_test = vec_for_learning(doc2vec_model, twenty_test_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training a classifier\n",
    "\n",
    "Finally, we can create a classifier and evaluate the results using performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Test accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Test F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Can you explain the scores? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
